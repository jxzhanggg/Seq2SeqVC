<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>seq2seqVC</title>
<style type="text/css">
</style></head>
<body>
<h2><strong>Audio Samples from &quot;Sequence-to-Sequence Acoustic Modeling for Voice Conversion</strong>&quot;</h2>
<h3>Paper:</h3>
<p><a href="https://arxiv.org/abs/1810.06865"> Sequence-to-Sequence Acoustic Modeling for Voice Conversion</a> (Submitted to IEEE/ACM Transactions on audio, speech and language processing, Aug. 2018)</p>
<h3>Authors: </h3>
Jing-Xuan Zhang, Zhen-Hua Ling, Li-Juan Liu, Yuan-Jiang, Li-Rong Dai
<h3>Abstract: </h3>
In this paper, a neural network named Sequence-to- sequence ConvErsion NeTwork (SCENT) is presented for acoustic modeling in voice conversion. At training stage, a SCENT model is estimated by aligning the feature sequences of source and target speakers implicitly using attention mechanism. At conversion stage, acoustic features and durations of source utterances are converted simultaneously using the unified acoustic model, which is difficult to be achieved in conventional method. Mel-scale spectrograms are adopted as acoustic features which contain both excitation and vocal tract descriptions of speech signals. The bottleneck features extracted from source speech using an automatic speech recognition (ASR) model are appended as auxiliary input. A WaveNet vocoder conditioned on Mel- spectrograms is built to reconstruct waveforms from the output of the SCENT model. Experimental results show that our proposed method achieved better objective and subjective performance than the baseline methods using Gaussian mixture models (GMM) and deep neural networks (DNN) as acoustic models. This proposed method also outperformed our previous work which achieved the top rank in Voice Conversion Challenge 2018. Ablation tests further confirm the effectiveness of appending bottleneck features and using attention module in our proposed method.
<p>&nbsp;</p>
<hr />
<h3>&nbsp;</h3>
<h3>0. Reference audios and corresponding texts in  Section 1, 2, 3</h3>
<table width="719" border="1">
  <tr>
    <td colspan="2">Female</td>
    <td colspan="2">Male</td>
  </tr>
  <tr>
    <td width="177"><audio controls>
        <source src="wav/truth/f/11.wav" type="audio/wav">
    </audio></td>
    <td width="163"><audio controls>
        <source src="wav/truth/f/20.wav" type="audio/wav">
    </audio></td>
    <td width="150"><audio controls>
        <source src="wav/truth/m/11.wav" type="audio/wav">
    </audio></td>
    <td width="201"><audio controls>
        <source src="wav/truth/m/20.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td height="67">    
    <p>他难道不知道我要存钱去超市买零食啊？ (ta1 nan2 dao4 bu4 zhi1 dao4 wo3 yao4 cun2  qian2 qu4 chao1 shi4 mai3 ling2 shi2 a0?)</p></td>
    <td><p>马上停机了，明天要是去市里，记得给我充下话费。 (ma3 shang4 ting2 ji1 le0, ming2 tian1 yao4 shi4 qu4 shi4 li3, ji4 de0 gei2 wo3 chong1 xia3 hua4 fei4.)</p></td>
    <td><p>他难道不知道我要存钱去超市买零食啊？ (ta1 nan2 dao4 bu4 zhi1 dao4 wo3 yao4 cun2  qian2 qu4 chao1 shi4 mai3 ling2 shi2 a0?)</p></td>
    <td>马上停机了，明天要是去市里，记得给我充下话费。 (ma3 shang4 ting2 ji1 le0, ming2 tian1 yao4 shi4 qu4 shi4 li3, ji4 de0 gei2 wo3 chong1 xia3 hua4 fei4.)</td>
  </tr>
</table>
<p>&nbsp;</p>
<h3>1. Comparsion of proposed method to baseline methods</h3>
<p><strong>Samples of experiments on Mandarin dataset.</strong></p>
<table width="763" border="1">
  <tr>
    <td width="97" nowrap="nowrap">Methods</td>
    <td colspan="2">Female-to-Male</td>
    <td colspan="2">Male-to-Female</td>
  </tr>
  <tr>
    <td nowrap="nowrap">i-JD-GMM</td>
    <td><audio controls>
        <source src="wav/i-GMM/f-m/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i-GMM/f-m/20.wav" type="audio/wav">
      </audio></td>
    <td><audio controls>
        <source src="wav/i-GMM/m-f/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i-GMM/m-f/20.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td nowrap="nowrap">i-bn-DNN</td>
    <td><audio controls>
        <source src="wav/i-bn-DNN/f-m/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i-bn-DNN/f-m/20.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i-bn-DNN/m-f/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i-bn-DNN/m-f/20.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td nowrap="nowrap">i-VCC2018</td>
    <td><audio controls>
        <source src="wav/i-vcc2018/f-m/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i-vcc2018/f-m/20.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i-vcc2018/m-f/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i-vcc2018/m-f/20.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td nowrap="nowrap"><em>Proposed</em></td>
    <td><audio controls>
        <source src="wav/proposed/f-m/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/proposed/f-m/20.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/proposed/m-f/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/proposed/m-f/20.wav" type="audio/wav">
    </audio></td>
  </tr>
</table>
<p>&quot;i-&quot; in the prefixes of method names represent interpolation for global speaking rate adjustment. &quot;-bn-&quot; represents using bottleneck features for DNN based method as we described in the paper.</p>
<p><strong>Samples of experiments on English CMU ARCTIC dataset.</strong></p>
<table width="771" border="1">
  <tr>
    <td width="112">Methods</td>
    <td colspan="2">SLT-to-RMS</td>
    <td colspan="2">RMS-to-SLT</td>
  </tr>
  <tr>
    <td nowrap="nowrap">i-JD-GMM</td>
    <td width="149"><audio controls>
        <source src="wav/demo_arctic/slt2rms/i-GMM/00151.wav" type="audio/wav">
    </audio></td>
    <td width="161"><audio controls><source src="wav/demo_arctic/slt2rms/i-GMM/00410.wav" type="audio/wav">
    </audio></td>
    <td width="150"><audio controls>
        <source src="wav/demo_arctic/rms2slt/i-GMM/00151.wav" type="audio/wav">
    </audio></td>
    <td width="165"><audio controls>
        <source src="wav/demo_arctic/rms2slt/i-GMM/00410.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td>i-bn-DNN</td>
    <td><audio controls>
        <source src="wav/demo_arctic/slt2rms/i-bn-DNN/00151.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/slt2rms/i-bn-DNN/00410.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/rms2slt/i-bn-DNN/00151.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/rms2slt/i-bn-DNN/00410.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td nowrap="nowrap">i-VCC2018</td>
    <td><audio controls>
        <source src="wav/demo_arctic/slt2rms/i-VCC2018/00151.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/slt2rms/i-VCC2018/00410.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/rms2slt/i-VCC2018/00151.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/rms2slt/i-VCC2018/00410.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td><em>Proposed</em></td>
    <td><audio controls>
        <source src="wav/demo_arctic/slt2rms/seq2seq/00151.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/slt2rms/seq2seq/00410.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/rms2slt/seq2seq/00151.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/rms2slt/seq2seq/00410.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td>Reference</td>
    <td><audio controls>
        <source src="wav/demo_arctic/slt2rms/truth/00151.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/slt2rms/truth/00410.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/rms2slt/truth/00151.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/demo_arctic/rms2slt/truth/00410.wav" type="audio/wav">
    </audio></td>
  </tr>
</table>
<p>&quot;i-&quot; in the prefixes of method names represent interpolation for global speaking rate adjustment. &quot;-bn-&quot; represents using bottleneck features for DNN based method as we described in the paper.</p>
<p>&nbsp;</p>
<h3><em>2. Comparison of proposed method to proposed method without using Mel spectrograms and bottleneck features</em></h3>
<table width="764" border="1">
  <tr>
    <td width="97" nowrap="nowrap">Methods</td>
    <td colspan="2">Female-to-Male</td>
    <td colspan="2">Male-to-Female</td>
  </tr>
  <tr>
    <td nowrap="nowrap">Proposed</td>
    <td width="157"><audio controls>
        <source src="wav/proposed/f-m/11.wav" type="audio/wav">
      </audio></td>
    <td width="157"><audio controls>
        <source src="wav/proposed/f-m/20.wav" type="audio/wav">
      </audio></td>
    <td width="161"><audio controls>
        <source src="wav/proposed/m-f/11.wav" type="audio/wav">
      </audio></td>
    <td width="158"><audio controls>
        <source src="wav/proposed/m-f/20.wav" type="audio/wav">
      </audio></td>
  </tr>
  <tr>
    <td nowrap="nowrap">w/o-Mel</td>
    <td><audio controls>
        <source src="wav/no_mel/l2g_tanan.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/no_mel/l2g_mashang.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/no_mel/g2l_tanan.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/no_mel/g2l_mashang.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td nowrap="nowrap">w/o-bn</td>
   <td><audio controls>
        <source src="wav/w_o_bn/f-m2/gen_spc_13_wavnet.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/w_o_bn/f-m2/gen_spc_28_wavnet.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/w_o_bn/m-f2/gen_spc_13_wavnet.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/w_o_bn/m-f2/gen_spc_28_wavnet.wav" type="audio/wav">
    </audio></td>
  </tr>
</table>
<p>&quot;w/o-Mel&quot; and &quot;w/o-bn&quot; represent proposed method without using Mel spectrograms and bottleneck features respectively.</p>
<p>&nbsp;</p>
<p><strong>Adding bottleneck features is beneficial for improving the pronunciation correctness of converted speech.</strong></p>
<table width="775" border="1">
  <tr>
    <td width="337">Methods</td>
    <td width="422">Audio samples</td>
    <td width="422">Texts sound like</td>
  </tr>
  <tr>
    <td>Proposed</td>
    <td><audio controls>
        <source src="wav/demo_bn/seq2seq.wav" type="audio/wav">
    </audio></td>
    <td>想听音乐，您就说打开音乐。（xiang3 ting1 yin1 yue4, nin2 jiu4 shuo1 da3 ka1 yin1 yue4.）</td>
  </tr>
  <tr>
    <td>w/o-bn</td>
    <td><audio controls>
        <source src="wav/demo_bn/seq2seq_nobn.wav" type="audio/wav">
    </audio></td>
    <td>想听音乐，就说打开音乐。（xiang3 ting1 yin1 <font color="#FF0000"><em>（unclear voice）, (skipping phoneme)</em></font> jiu4 shuo1 da3 ka1 yin1 yue4.）</td>
  </tr>
  <tr>
    <td>Reference</td>
    <td><audio controls>
        <source src="wav/demo_bn/00000697.wav" type="audio/wav">
    </audio></td>
    <td>想听音乐，您就说打开音乐。（xiang3 ting1 yin1 yue4, nin2 jiu4 shuo1 da3 ka1 yin1 yue4.）</td>
  </tr>
</table>
<p>&nbsp;</p>
<h3>3. Comparison of proposed method  to proposed method without attention module</h3>
<table width="768" border="1">
  <tr>
    <td width="93" nowrap="nowrap">Methods</td>
    <td colspan="2">Female-to-Male</td>
    <td colspan="2">Male-to-Female</td>
  </tr>
  <tr>
    <td nowrap="nowrap">Proposed</td>
        <td width="172"><audio controls>
        <source src="wav/proposed/f-m/11.wav" type="audio/wav">
      </audio></td>
    <td width="153"><audio controls>
        <source src="wav/proposed/f-m/20.wav" type="audio/wav">
      </audio></td>
    <td width="161"><audio controls>
        <source src="wav/proposed/m-f/11.wav" type="audio/wav">
      </audio></td>
    <td width="155"><audio controls>
        <source src="wav/proposed/m-f/20.wav" type="audio/wav">
      </audio></td>
  </tr>
  <tr>
    <td nowrap="nowrap">w/o-att</td>
    <td><audio controls>
        <source src="wav/w_o_att/f-m/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/w_o_att/f-m/20.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/w_o_att/m-f/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/w_o_att/m-f/20.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td nowrap="nowrap">i-w/o-att</td>
    <td><audio controls>
        <source src="wav/i_w_o_att/f-m/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i_w_o_att/f-m/20.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i_w_o_att/m-f/11.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/i_w_o_att/m-f/20.wav" type="audio/wav">
    </audio></td>
  </tr>
</table>
<p> ''w/o-att'' represents proposed method without attention module; ''i-w/o-att&quot; represents proposed method without attention and using interpolation for global speaking rate adjustment.</p>
<p>&nbsp;</p>
<h3><em>4. Comparsion of proposed method to proposed method without location code</em></h3>
<table width="775" border="1">
  <tr>
    <td width="104">Methods</td>
    <td colspan="2">Female-to-Male</td>
    <td colspan="2">Male-to-Female</td>
  </tr>
  <tr>
    <td>Proposed</td>
    <td width="165"><audio controls>
        <source src="wav/proposed/f-m/11.wav" type="audio/wav">
      </audio></td>
    <td width="152"><audio controls>
        <source src="wav/proposed/f-m/20.wav" type="audio/wav">
    </audio></td>
    <td width="176"><audio controls>
        <source src="wav/proposed/m-f/11.wav" type="audio/wav">
    </audio></td>
    <td width="144"><audio controls>
        <source src="wav/proposed/m-f/20.wav" type="audio/wav">
    </audio></td>
  </tr>
  <tr>
    <td>w/o-locc</td>
    <td><audio controls>
        <source src="wav/no_pos/l2g_tanan.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/no_pos/l2g_mashang.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/no_pos/g2l_tanan.wav" type="audio/wav">
    </audio></td>
    <td><audio controls>
        <source src="wav/no_pos/l2g_mashang.wav" type="audio/wav">
    </audio></td>
  </tr>
</table>
<p>&quot;w/o-locc&quot; represents proposed method without using location code.</p>
<p>&nbsp;</p>
<h3><em>5. Examples of mispronunciation  in proposed method</em></h3>
<table width="1155" border="1">
  <tr>
    <td width="89" nowrap="nowrap">Conversion Pairs</td>
    <td width="248">Source utterances</td>
    <td width="263">Ground truth texts</td>
    <td width="236">Converted utterances</td>
    <td width="285">Texts sound like </td>
  </tr>
  <tr>
    <td nowrap="nowrap">Female-to-Male</td>
    <td><audio controls>
        <source src="wav/failed/fm/00000014.wav" type="audio/wav">
    </audio></td>
    <td width="263"><p>懒鬼起床了，大懒虫，太阳晒屁股了，该运动运动了。(lan2 gui3 qi3 chuang2 le0, da4 lan3 chong2, tai4 yang2 sha4 pi4 gui0 le0, gai1 yun4 dong0 yun4 dong0 le0.)</p>
    </td>
    <td><audio controls>
        <source src="wav/failed/fm/00000014c.wav" type="audio/wav">
    </audio></td>
    <td width="285">lan2 gui3 qi3 <font color="#FF0000"><em>chang</em></font>2 le0, da4 lan3 chong2, tai4 yang2 sha4 pi4 gui0 le0, gai1 yun4 dong0 yun4 dong0 le0.</td>
  </tr>
  <tr>
    <td nowrap="nowrap">Female-to-Male</td>
    <td><audio controls>
        <source src="wav/failed/fm/00000108.wav" type="audio/wav">
    </audio></td>
    <td width="263"><p>做作业从早上八点半到晚上十二点。(zuo4 zuo4 ye4 cong2 zao3 shang4 ba1 dian3 ban4 dao4 shi2 er4 dian3.)</p></td>
    <td><audio controls>
        <source src="wav/failed/fm/00000108c.wav" type="audio/wav">
    </audio></td>
    <td width="285">zuo4 zuo4 ye4 cong2 zao3 shang4 ba1 <font color="#FF0000"><em>dan</em></font>4 ban4 dao4 shi2 er4 dian3.</td>
  </tr>
  <tr>
    <td nowrap="nowrap">Male-to-Female</td>
    <td><audio controls>
        <source src="wav/failed/mf/00000203.wav" type="audio/wav">
    </audio></td>
    <td width="263"><p>想娶我吗?.那是要付出代价的。 (xiang2 qu2 wo3 ma0? na4 shi4 yao4 fu4 chu1 dai4 jia0 de0.)</p></td>
    <td><audio controls>
        <source src="wav/failed/mf/00000203c.wav" type="audio/wav">
    </audio></td>
    <td width="285">xiang2 qu2 wo3 ma0? na4 shi4 yao4 fu<font color="#FF0000"><em>3</em></font> chu1 dai4 jia<font color="#FF0000"><em>3</em></font> de0.</td>
  </tr>
  <tr>
    <td nowrap="nowrap">Male-to-Female</td>
    <td><audio controls>
        <source src="wav/failed/mf/00000209.wav" type="audio/wav">
    </audio></td>
    <td width="263"><p>你家最近怎么这么多客人呢？(ni3 jia1 zui4 jin4 zen3 me0 zhe4 me0 duo1 ke4 ren0 ne0?)</p></td>
    <td><audio controls>
        <source src="wav/failed/mf/00000209c.wav" type="audio/wav">
    </audio></td>
    <td width="285">ni3 jia1 <font color="#FF0000"><em>zi</em></font>4 jin4 zen3 me0 zhe4 me0 duo1 ke<font color="#FF0000"><em>3</em></font> ren<font color="#FF0000"><em>2</em></font> ne0?</td>
  </tr>
</table>
<p>Notice that these samples are selected from the extras non-parallel part of datas<span class="red">ets, thus no </span>corresponding ground truth target utterances can be presented. Red emphasised phomenes or intonations indicate the incorrect pronunciation.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</body>
</html>
